format(dec1,format="%b %d %Y")
years=c(2004,2005,2006,2007,2008) rainfall=c(1500,1300,1800,1350,1950) plot(x=years,y=rainfall)
years=c(2004,2005,2006,2007,2008)rainfall=c(1500,1300,1800,1350,1950) plot(x=years,y=rainfall)
years=c(2004,2005,2006,2007,2008)
rainfall=c(1500,1300,1800,1350,1950)
plot(x=years,y=rainfall)
thisdata=data.frame(years,rainfall)
bestfit=lm(rainfall years,data=thisdata)
?lm
bestfit=lm(rainfall~years,data=thisdata)
bestfit
demo(graphics)
l
ls()
search()
rm()
ls()
?rm
rm(ls())
rm(list=ls())
ls()
dete.mdy(sdate=15000)
library(date)
library(date)
require(date)
install.packages("date")
library(date)
dete.mdy(sdate=15000)
date.mdy(sdate=15000)
source('/Volumes/JHBae-Data/My Works/[BookCD]/BeginersRcourseKorean/quadrats.r')
#Normally, at the start of an R program you have the various function(s) defined that are used by the program.
#This has to come before the MAIN PROGRAM (which is where your program really starts).
cstest=function(observed,expected,P,doftosubtract) {
brief=FALSE	##For a less detailed report of this test, change this to "brief=TRUE"
if (length(observed)!=length(expected) || !is.finite(sum(observed)) || !is.finite(sum(expected))) {cat("Tidying up data.\n")}
obs=c();exd=c()
for (i in 1:min(length(observed),length(expected))) {
if (is.finite(observed[i]) && is.finite(expected[i])) {obs=c(obs,observed[i]);exd=c(exd,expected[i])}
}
nu=length(obs)-doftosubtract	#nu is the number of degrees of freedom. Subtract 1 if the expected values came from a source independent of the observed values, subtract >1 if you have a contingency table with more than one column or the expected values are derived from e.g. a model fit.
if (nu<1) {
cat("No degrees of freedom - can't do test.\n")
} else {
alpha=(100-P)/100
chisqthresh=qchisq(1-alpha,df=nu)	#this command replaces the old method of looking up the threshold in a statistical table. For me it's not necessary to know anything about the chi-squared distribution itself, but there's an illustration of it at http://www.statsoft.com/textbook/statistics-glossary/c/button/c/#Chi-square%20Distribution if you want to see what's behind this command.
if (!brief) {cat("Threshold is: chi^2_(",alpha,",",nu,") =",chisqthresh,"\n")}
if (nu<2) {
cat("Applying Yates's correction.\n")
chisqstat=sum(((abs(obs-exd)-0.5)^2)/exd)
} else {
chisqstat=sum(((obs-exd)^2)/exd)
}
if (!brief) {cat("Test statistic is: Chi^2 =",chisqstat,"\n")}
pvalue=1-pchisq(chisqstat,df=nu)
pvaluetext1=paste("p=",pvalue)
if (pvalue>0.1) {pvaluetext1="p>0.1"}
if (pvalue<0.001) {pvaluetext1="p<0.001"}
pvaluetext2="not significant"
if (pvalue<0.1) {pvaluetext2="weakly significant"}
if (pvalue<0.05) {pvaluetext2="significant"}
if (pvalue<0.01) {pvaluetext2="highly significant"}
if (pvalue<0.001) {pvaluetext2="very highly significant"}
if (!brief) {cat("At the",P,"% significance level (ie. you're saying that you're only going to reject a null model if there's a <",100-P,"%\nchance that you'll be wrong due to the results being 'coincidence' or 'unusual' (which is always a possibility when\nmaking generalisations about a population from measurements of a sample)):\n")}
if (chisqstat<chisqthresh) {	#equivalent to "if (pvalue>alpha) {"
if (!brief) {cat("\tThe observed values **AGREE** with those predicted by the null hypothesis (i.e. this is not enough evidence to reject the\nnull hypothesis; the data fits the null hypothesis tolerably well; the deviation of observed values from the null hypothesis can be\nexplained by chance alone; observations are what was expected).\n")}
cat("Observations are as expected, so null hypothesis accepted (chi-squared test, n=",length(obs),",",pvaluetext1,").\n")
} else {
if (!brief) {cat("\tThe observed values **ARE DIFFERENT** from those predicted by the null hypothesis (ie. the data doesn't support the\nnull hypothesis; the null hypothesis should be rejected; consider alternative hypotheses)\n")}
if (!brief) {cat("\t(p value for this result is",pvalue,"- a",pvaluetext2,"rejection of the null hypothesis).\n")}
if (mean(obs)<mean(exd)) {cat("Observations are generally lower than expected,")} else {cat("Observations are generally higher than expected,")}
cat(" so null hypothesis rejected (chi-squared test, n=",length(obs),",",pvaluetext1,").\n")
}
}
}
############################
####### MAIN PROGRAM #######
############################
filedata=read.table("quadratdata",header=TRUE)
#reading from file "quadratdata" (just to demonstrate how to do that). Some people use read.delim(...) or read.csv(...), which are basically the same as read.table(...,sep="\t") and read.table(...,sep=","). Sometimes, R reads in a column of numbers, but doesn't recognise the values and labels them as 'factors': if this happens to you, try the solution at https://stat.ethz.ch/pipermail/r-help/2000-February/010165.html.
categories=as.vector(t(filedata[1]))
filedata=read.table("quadratdata",header=TRUE)
#reading from file "quadratdata" (just to demonstrate how to do that). Some people use read.delim(...) or read.csv(...), which are basically the same as read.table(...,sep="\t") and read.table(...,sep=","). Sometimes, R reads in a column of numbers, but doesn't recognise the values and labels them as 'factors': if this happens to you, try the solution at https://stat.ethz.ch/pipermail/r-help/2000-February/010165.html.
categories=as.vector(t(filedata[1]))
#equivalent of categories=c("0","1","2","3","4","5","6","7","8+")
#these categories MUST be mutually exclusive AND exhaustive
observed=as.vector(t(filedata[2]))
#equivalent of observed=c(6,23,25,43,59,54,27,13,10)
#these observed counts must be count data (ie. integers) for this test
if (length(categories)!=length(observed)) {stop("There's something up with the categories and/or observed counts.")}
checkcount=0;for (i in 1:length(observed)) {if (observed[i]<5) {checkcount=checkcount+1}}
if (min(observed)<2 || checkcount>(length(observed)*20/100)) {stop("This result is invalid because you've got too few counts.\nYou need a) all of them >1 and b) less than 20% of them <5 otherwise you can't use a chi-squared test.\nTry grouping your data in to larger categories.\n")}
numquadrats=sum(observed)
estofmean=sum(observed*c(0,1,2,3,4,5,6,7,10))/numquadrats	#assuming mean of the 8+ category is 10.
#I think the hardest bit is always working out the expected values:
probsA=c(rep(0,times=length(observed)))
tmp=trunc(estofmean);if (tmp>(length(observed)-1)) {tmp=length(observed)-1}
probsA[tmp+1]=1			#for a uniform distribution of trees, all quadrats should contain (estofmean) trees
tmp=dpois(0:(length(observed)-2),lambda=estofmean)
probsB=c(tmp,1-sum(tmp))	#for a random distribution, the numbers come from a Poisson distribution
expectedA=numquadrats*probsA
expectedB=numquadrats*probsB
#Now some explanation to the user of what's going on:
cat("You are an ecologist and you have set out some randomly and independently-positioned quadrats, each 10m by 10m, in a\nforest. You have counted the number of trees in each quadrat and your results are in the 'Observed' column of this\ntable (no. of trees in the first column) and the bar plot:\n")
table=data.frame(observed,expectedA,expectedB)
rownames(table)=categories
colnames(table)=c("Observed counts","Expected (A)","Expected (B)")
print(table)
bpmatrix=t(as.matrix(table))
barplot(bpmatrix,beside=TRUE,xlab="no. of trees",ylab="counts",legend=colnames(table))
cat("Total number of quadrats:",numquadrats,"\nEstimated mean number of trees per quadrat (assuming mean of the 8+ category is 10):",estofmean,"\n")
cat("You want to use a chi-squared 'goodness-of-fit' test to assess two null models: a uniform distribution (A) and a\nrandom distribution (B) (your alternative model/theory/hypothesis might be, for example, a clumped distribution). I've\nput the expected values from each of these theories in the table and the bar plot too.\n")
cat("Press a <ENTER> or <RETURN> to continue ");tmp=scan(n=1,quiet=TRUE)
P=95			#using a confidence level of P% (only use 95% or 99% in publications)
cat("\nWith A as the null theory/model/hypothesis:\n")
cstest(observed,expectedA,P,2)	#need to subtract 1 degree of freedom anyway for chi squared, but an extra one because estofmean was used in deriving the expected values
cat("Press a <ENTER> or <RETURN> to continue ");tmp=scan(n=1,quiet=TRUE)
cstest(observed,expectedB,P,2)
#Final comment about the way I've done the test above:
#Most R-users would do a chi-squared test using an in-built R function like this one:
#   pvalue=chisq.test(x=observed,p=expected,rescale.p=TRUE)$p.value
#(read the ?chisq.test manual page for more about how to use this). I'm aware of this, but I generally don't do things that way because all the calculations are hidden from me. It's a shame that R is not written with a 'verbose' option that goes through the calculations step-by-step for you with explanations, but that's the way life is. Why do I need to know what's going on? So that I can a) explain how the test works to someone else and b) if you ever have to do a nonstandard analysis you'll have to learn all about the standard ones in depth anyway.
library("Benchmarking", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
lpSolveAPI
lpSolveAPI
Benchmarking
??Benchmarking
ls()
P
?ls
?rm
rm(list=ls())
ls()
citation()
rep(2,5)
matrix(rep(2,12),ncol=3)
A=matrix(rep(2,12),ncol=3)
A
matrix(A)
diag(A)
seq(from=5, to=4, by=0.5)
?seq
seq(from=3, to=4, by=0.5)
seq(3,4, by=0.5)
seq(3,4,0.5)
seq(3,4,length.out=8)
A<-seq(3,4,length.out=8)
A
rep(A,each=2)
M=matrix(c(1,2,3,4,5,6), ncol=2)
M=matrix(c(1,2,3,4,5,6), ncol=2, byrow=TRUE)
attribute(M)
attribute(M)
attributes(M)
dim(M)
diag(c(1,2,3,4))
A=dim(M)
A
A[1]
A[]
dim(M)[]
dim(M)[2]
M
A=set.seed(2)
A
A=seed(20)
A<-set.seed(30)
A
attributes(A)
class(A)
et.seed(1)
x <- sample(1:10, 25, replace = TRUE)
set.seed(1)
x <- sample(1:10, 25, replace = TRUE)
x
?sample
set.seed(5)
x <- sample(1:10, 25, replace = TRUE)
x
rm(list=rm())
x <- sample(1:10, 25, replace = TRUE)
x
?set.seed
set.seed(1)
x <- sample(1:10, 25, replace = TRUE)
x
x <- sample(1:10, 25, replace = TRUE)
x
?normal
??normal
??'normal distribution'
rnorm(100, mean=0, sd=1)
rnorm(100, mean=0, sd=1)
hist(x)
x=rnorm(100, mean=0, sd=1)
hist(x)
x=rnorm(10000, mean=0, sd=1)
hist(x)
require(graphics)
dnorm(0) == 1/sqrt(2*pi)
dnorm(1) == exp(-1/2)/sqrt(2*pi)
dnorm(1) == 1/sqrt(2*pi*exp(1))
## Using "log = TRUE" for an extended range :
par(mfrow = c(2,1))
plot(function(x) dnorm(x, log = TRUE), -60, 50,
main = "log { Normal density }")
curve(log(dnorm(x)), add = TRUE, col = "red", lwd = 2)
mtext("dnorm(x, log=TRUE)", adj = 0)
mtext("log(dnorm(x))", col = "red", adj = 1)
plot(function(x) pnorm(x, log.p = TRUE), -50, 10,
main = "log { Normal Cumulative }")
curve(log(pnorm(x)), add = TRUE, col = "red", lwd = 2)
mtext("pnorm(x, log=TRUE)", adj = 0)
mtext("log(pnorm(x))", col = "red", adj = 1)
## if you want the so-called 'error function'
erf <- function(x) 2 * pnorm(x * sqrt(2)) - 1
## (see Abramowitz and Stegun 29.2.29)
## and the so-called 'complementary error function'
erfc <- function(x) 2 * pnorm(x * sqrt(2), lower = FALSE)
## and the inverses
erfinv <- function (x) qnorm((1 + x)/2)/sqrt(2)
erfcinv <- function (x) qnorm(x/2, lower = FALSE)/sqrt(2)
plot(function(x) dnorm(x, log = FALSE), -60, 50,
+      main = "log { Normal density }")
plot(function(x) dnorm(x, log = FALSE), -60, 50,
+      main = "log { Normal density }")
plot(function(x) dnorm(x, log = FALSE), -60, 50,
+      main = "log{Normal Denst.}"
x
function(x)
rnormal(x)
?log
q()
require(pmr)
install.packages("pmr")
require(pmr)
abc <- matrix(data = 1:15, nrow = 4, ncol = 4, byrow = TRUE)
abc[1,1] <- 1
abc[1,2] <- 2
abc[1,3] <- 5
abc[1,4] <- 4
abc[2,1] <- 1/2
abc[2,2] <- 1
abc[2,3] <- 3
abc[2,4] <- 1.9
abc[3,1] <- 1/5
abc[3,2] <- 1/3
abc[3,3] <- 1
abc[3,4] <- 0.7
abc[4,1] <- 1/4
abc[4,2] <- 1/1.9
abc[4,3] <- 1/0.7
abc[4,4] <- 1
ahp(abc)
?ahp
ahp(abc, sim_size=300)
load('pmr')
library('pmr')
ahp(abc)
## ahp(abc)
ahp
ahp
?pmr
pmr
pmr
load.package(pmr)
load.packages(pmr)
library('pmr')
require('pmr')
pmr
ls()
ahp
library('pmr')
ahp(abc)
ahp('abc')
?ahp
citation()
rm(list=ls())
require('pmr')
abc <- matrix(data = 1:15, nrow = 4, ncol = 4, byrow = TRUE)
abc <- matrix(data = 1:16, nrow = 4, ncol = 4, byrow = TRUE)
abc <- matrix(data = 1:16, nrow = 4, ncol = 4, byrow = TRUE)
abc
abc[1,1] <- 1
abc[1,2] <- 2
abc[1,3] <- 5
abc[1,4] <- 4
abc[2,1] <- 1/2
abc[2,2] <- 1
abc[2,3] <- 3
abc[2,4] <- 1.9
abc[3,1] <- 1/5
abc[3,2] <- 1/3
abc[3,3] <- 1
abc[3,4] <- 0.7
abc[4,1] <- 1/4
abc[4,2] <- 1/1.9
abc[4,3] <- 1/0.7
abc[4,4] <- 1
abc
ahp(abc)
install.packages("knitr")
library("codetools", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
install.packages(c("car", "class", "evaluate", "formatR", "gdata", "gtools", "Hmisc", "knitr", "labeling", "markdown", "MASS", "mgcv", "munsell", "mvtnorm", "nlme", "nnet", "pmr", "rgl", "rms", "RODBC", "spatial", "zoo"))
q()
setwd("/Volumes/JHBae-Data/Dropbox/QCData/20131030")
dataFile <- getFileName()
source("getFileName.r")
source("loadData.r")
dataFile <- getFileName()
dataFile <- getFileName()
setwd("/Volumes/JHBae-Data/Dropbox/QCData/20131011/Correlations Analysis")
dataFile <- getFileName()
# What is the file Name? RegressionModel.xlsx
rawData <- loadData(dataFile)
regData <- rawData
linearReg <- lm(regData$Col.125 ~ regData$MDL05, regData)
summary(linearReg)
plot(linearReg)
mLinearReg <- lm(regData$Col.125 ~ regData$MDM01 + regData$MDM02 + regData$MDM03, regData)
summary(mLinearReg)
plot(mLinearReg)
library(Hmisc)
rcorr(as.matrix(regData[,2:7]), type="spearman")
pdf("Corelations Analysis.pdf", paper="a4r", width=9.7)
qcc(regData$Col.125, type="xbar.one")
qcc(regData$MDL05, type="xbar.one")
qcc(regData$MDM01, type="xbar.one")
qcc(regData$MDM02, type="xbar.one")
qcc(regData$MDM03, type="xbar.one")
qcc(regData$MDH01, type="xbar.one")
plot(linearReg)
plot(mLinearReg)
dev.off()
library("qcc")
pdf("Corelations Analysis.pdf", paper="a4r", width=9.7)
qcc(regData$Col.125, type="xbar.one")
qcc(regData$MDL05, type="xbar.one")
qcc(regData$MDM01, type="xbar.one")
qcc(regData$MDM02, type="xbar.one")
qcc(regData$MDM03, type="xbar.one")
qcc(regData$MDH01, type="xbar.one")
plot(linearReg)
plot(mLinearReg)
dev.off()
rcorr(as.matrix(regData[,2:7]), type="spearman")
rcorr(as.matrix(regData[,2:7]), type="pearson")
mydata <- rawData[, 2:7]
fit <- princomp(mydata, cor=TRUE)
summary(fit) # print variance accounted for
loadings(fit) # pc loadings
plot(fit,type="lines") # scree plot
fit$scores # the principal components
biplot(fit)
mydata <- rawData[, 2:7]
fit <- princomp(mydata, cor=TRUE)
summary(fit) # print variance accounted for
loadings(fit) # pc loadings
fit <- principal(mydata, nfactors=5, rotate="varimax")
library(psych)
fit <- principal(mydata, nfactors=5, rotate="varimax")
fit
fit <- principal(mydata, nfactors=4, rotate="varimax")
fit
fit <- principal(mydata, nfactors=3, rotate="varimax")
fit
fit <- principal(mydata, nfactors=2, rotate="varimax")
fit
fit <- principal(mydata, nfactors=1, rotate="varimax")
fit
plot(fit, type="lines")
plot(fit$values, type="lines")
plot(fit,type="lines")
fit$values
warnings()
fit$values
fit <- principal(mydata, nfactors=5, rotate="varimax")
fit
A <- matrix(c(16, 15, 14, 15, 14, 14, 17, 16, 15, 17, 18, 18, 62, 60, 59, 61, 60, 59, 63, 62, 60, 63, 62, 64, 67, 69, 68, 71, 70, 69, 68, 69, 72, 69, 68, 67, 27, 31,31,31,30, 30, 29, 28, 30, 27, 28, 29), ncol=4)
rcorr(A, type="pearson")
rcorr(A, type="spearman")
rcorr(mydata, type="pearson")
rcorr(as.matrix(mydata), type="pearson")
X1 = matrix(c(72, 56, 55, 44, 97, 83, 47, 88, 57, 26, 46,
49, 71, 71, 67, 55, 49, 72, 61, 35, 84, 87, 73, 80, 26, 89, 66,
50, 47, 39, 27, 62, 63, 58, 69, 63, 51, 80, 74, 38, 79, 33, 22,
54, 48, 91, 53, 84, 41, 52, 63, 78, 82, 69, 70, 72, 55, 61, 62,
41, 49, 42, 60, 74, 58, 62, 58, 69, 46, 48, 34, 87, 55, 70, 94,
49, 76, 59, 57, 46), ncol = 4)
X2 = matrix(c(23, 14, 13, 9, 36, 30, 12, 31, 14, 7, 10,
11, 22, 21, 18, 15, 13, 22, 19, 10, 30, 31, 22, 28, 10, 35, 18,
11, 10, 11, 8, 20, 16, 19, 19, 16, 14, 28, 20, 11, 28, 8, 6,
15, 14, 36, 14, 30, 8, 35, 19, 27, 31, 17, 18, 20, 16, 18, 16,
13, 10, 9, 16, 25, 15, 18, 16, 19, 10, 30, 9, 31, 15, 20, 35,
12, 26, 17, 14, 16), ncol = 4)
X = list(X1 = X1, X2 = X2)
X
q = mqcc(X, type = "T2")
summary(q)
data(boiler)
boiler
q = mqcc(boiler, type = "T2.single", confidence.level = 0.999)
q2 = mqcc(mydata, type = "T2.single", confidence.level = 0.999)
mydata
library(MASS)
rob = cov.rob(boiler)
qrob = mqcc(boiler, type = "T2.single", center = rob$center, cov = rob$cov)
summary(qrob)
summary(q2)
rcorr(mydata[,2:6])
rcorr(as.matrix(mydata[,2:6]), corr=TRUE)
rcorr(as.matrix(mydata[,2:6]), type="pearson")
fit2 <- principal(mydata[,2:6], nfactors=3, rotate="varimax")
fit2
biplot(fit2)
mydata <- rawData[, 2:7]
fit <- princomp(mydata, cor=TRUE)
summary(fit) # print variance accounted for
loadings(fit) # pc loadings
plot(fit,type="lines") # scree plot
fit$scores # the principal components
biplot(fit)
plot(fit,type="lines") # scree plot
dataFile <- getFileName()
# What is the file Name? RegressionModel.xlsx
rawData <- loadData(dataFile)
regData <- rawData
linearReg <- lm(regData$Col.125 ~ regData$MDL05, regData)
summary(linearReg)
plot(linearReg)
mLinearReg <- lm(regData$Col.125 ~ regData$MDM01 + regData$MDM02 + regData$MDM03, regData)
summary(mLinearReg)
plot(mLinearReg)
library(Hmisc)
rcorr(as.matrix(regData[,2:7]), type="spearman")
rcorr(as.matrix(regData[,2:7]), type="pearson")
mydata <- rawData[, 2:7]
fit <- princomp(mydata, cor=TRUE)
summary(fit) # print variance accounted for
loadings(fit) # pc loadings
plot(fit,type="lines") # scree plot
fit$scores # the principal components
biplot(fit)
A
fit2 <- princomp(A, cor=TRUE)
summary(fit2)
biplot(fit2)
loadings(fit2)
linearA <- lm(A[1]~a[2], A)
A
linearA <- lm(A[,1]~a[,2], A)
linearA <- lm(as.df(A[,1])~as.df(a[,2]), A)
as.dataframe
?as.df
linearA <- lm(as.data.frame(A[,1])~as.data.frame(a[,2]), A)
as.data.frame(A[,1])
linearA <- lm(as.data.frame(A[,1])~as.data.frame(A[,2])), A)
linearA <- lm(as.data.frame(A[,1])~as.data.frame(A[,2])), data=A)
linearA <- lm(as.data.frame(A[,1])~as.data.frame(A[,2])))
linearA <- lm(as.data.frame(A[,1])~as.data.frame(A[,2]), A)
A$"1"
dfA <- as.data.frame(A)
dfA
dfA$V1
linearA <- lm(dfA$V1~dfA$V2, dfA)
summary(linearA)
linearReg <- lm(regData$Col.125 ~ regData$MDM01, regData)
summary(linearReg)
linearReg <- lm(regData$Col.125 ~ regData$MDM03, regData)
summary(linearReg)
mLinearReg <- lm(regData$Col.125 ~ regData$MDM01 + regData$MDM03, regData)
summary(mLinearReg)
size(regData[,1])
length(regData[,1])
seqNo <- 1:length(regData[,1])
plot(seqNo, regData[,1])
plot(seqNo, regData[,2])
plot(seqNo, regData[,2], type='b')
par(new="T")
plot(seqNo, regData[,3], type='b')
plot(seqNo, regData[,2], type='b', col=1)
par(new="T")
plot(seqNo, regData[,3], type='b', col=2)
plot(seqNo, regData[,4], type='b', col=3)
NewData <- cbind(regData$Col.125, regData$MDM01, regData$MDM05)
Hsq = mqcc(NewData, type = "T2.single", confidence.level = 0.999)
summary(HsQ)
summary(Hsq)
Hsq = mqcc(NewData[,2:3], type = "T2.single", confidence.level = 0.999)
NewData
NewData <- cbind(regData$Col.125, regData$MDM01, regData$MDM05)
NewData
regData$Col.125
' regData$MDM01'
regData$MDM01
regData$MDM05
NewData <- cbind(regData$Col.125, regData$MDM01, regData$MDM03)
Hsq = mqcc(NewData[,2:3], type = "T2.single", confidence.level = 0.999)
Hsq = mqcc(regData[,3:7], type = "T2.single", confidence.level = 0.999)
Hsq = mqcc(regData[,3:7], type = "T2", confidence.level = 0.999)
Hsq = mqcc(regData[,2:7], type = "T2", confidence.level = 0.999)
Hsq = mqcc(regData[,2:7], type = "T2", confidence.level = 0.9)
Hsq = mqcc(regData[,2:7], type = "T2", confidence.level = 0.96)
Hsq = mqcc(regData[,3:7], type = "T2.single", confidence.level = 0.9)
dataFile <- getFileName()
# What is the file Name? RegressionModel.xlsx
rawData <- loadData(dataFile)
regData <- rawData
mydata <- regData[,2:19]
T2MDO <- mqcc(mydata, type="T2.single", confidence.level=0.00)
T2MDO <- mqcc(mydata, type="T2.single", confidence.level=0.99)
head(mydata)
T2MDO <- mqcc(mydata[,1:12], type="T2.single", confidence.level=0.99)
